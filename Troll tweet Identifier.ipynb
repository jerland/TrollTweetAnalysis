{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing\n",
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"IRAhandle_tweets_1.csv\",sep = \",\", encoding =\"UTF8\" )\n",
    "df2 = pd.read_csv(\"IRAhandle_tweets_2.csv\",sep = \",\", encoding =\"UTF8\" )\n",
    "df3 = pd.read_csv(\"IRAhandle_tweets_3.csv\",sep = \",\", encoding =\"UTF8\" )\n",
    "df4 = pd.read_csv(\"IRAhandle_tweets_4.csv\",sep = \",\", encoding =\"UTF8\" )\n",
    "df5 = pd.read_csv(\"IRAhandle_tweets_5.csv\",sep = \",\", encoding =\"UTF8\" )\n",
    "df6 = pd.read_csv(\"IRAhandle_tweets_6.csv\",sep = \",\", encoding =\"UTF8\" )\n",
    "df7 = pd.read_csv(\"IRAhandle_tweets_7.csv\",sep = \",\", encoding =\"UTF8\" )\n",
    "df8 = pd.read_csv(\"IRAhandle_tweets_8.csv\",sep = \",\", encoding =\"UTF8\" )\n",
    "df9 = pd.read_csv(\"IRAhandle_tweets_9.csv\",sep = \",\", encoding =\"UTF8\" )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#df_complete = df1\n",
    "df_complete = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering dataset + Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_complete = df_complete[df_complete.language == \"English\"]\n",
    "df_complete = df_complete.drop([\"external_author_id\",\"author\",\"region\",\"language\",\"publish_date\",\"harvested_date\",\"following\",\"followers\",\"updates\",\"post_type\",\"account_type\",\"new_june_2018\",\"retweet\"], axis=1) #dropping every column besides category and content\n",
    "df_complete.drop(df_complete.loc[df_complete[\"account_category\"]==\"NonEnglish\"].index, inplace=True)\n",
    "df_complete.drop(df_complete.loc[df_complete[\"account_category\"]==\"Commercial\"].index, inplace=True)\n",
    "df_complete.drop(df_complete.loc[df_complete[\"account_category\"]==\"Unknown\"].index, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Deleting Links, Hashtags, @names and every other special character. Afterwards dropping dublicates\n",
    "df_complete[\"content\"] = df_complete[\"content\"].str.lower()\n",
    "df_complete[\"content\"] = df_complete['content'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "df_complete[\"content\"] = df_complete[\"content\"].replace(r\"#\\S+\\s*\",\"\", regex = True)\n",
    "df_complete[\"content\"] = df_complete[\"content\"].replace(r\"(^|[^\\w])@([\\w\\_\\.]+)\",\"\", regex = True)\n",
    "df_complete['content'] = df_complete['content'].str.replace(\"[^a-zA-Z ]\", \" \")\n",
    "df_complete = df_complete.drop_duplicates(subset=[\"content\"],keep=False)\n",
    "\n",
    "#removing stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "cleaned_tweets = []\n",
    "for index, row in df_complete.iterrows():\n",
    "    words_without_stopwords = [word for word in str(row.content).split() if not word in stopwords_set]\n",
    "    cleaned_tweets.append(\" \".join(words_without_stopwords))\n",
    "\n",
    "df_complete[\"content\"] = cleaned_tweets\n",
    "\n",
    "# Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "whitespace_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w,\"n\") for w in whitespace_tokenizer.tokenize(text)]\n",
    "df_complete[\"content\"] = df_complete.content.apply(lemmatize_text)\n",
    "df_complete[\"content_nontokenized\"] = df_complete[\"content\"].apply(\" \".join)\n",
    "\n",
    "\n",
    "df_filter = df_complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>account_category</th>\n",
       "      <th>content_nontokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[sitting, democrat, u, senator, trial, corrupt...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>sitting democrat u senator trial corruption ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[marshawn, lynch, arrives, game, anti, trump, ...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>marshawn lynch arrives game anti trump shirt j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[president, trump, dedicates, president, cup, ...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>president trump dedicates president cup golf t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[dan, bongino, nobody, troll, liberal, better,...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>dan bongino nobody troll liberal better donald...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[much, hate, promoting, cnn, article, admittin...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>much hate promoting cnn article admitting ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[genocide, remark, san, juan, mayor, narrative...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>genocide remark san juan mayor narrative chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[genocide, remark, san, juan, mayor, narrative...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>genocide remark san juan mayor narrative chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[apologize, u, lying]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>apologize u lying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[sarah, sander, destroys, nbc, reporter, trump...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>sarah sander destroys nbc reporter trump made ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[hi, remember, said, weinstein, wonderful, hum...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>hi remember said weinstein wonderful human goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[hi, remember, praised, harvey, weinstein, won...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>hi remember praised harvey weinstein wonderful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[wow, even, cnn, slamming, obamas, silence, we...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>wow even cnn slamming obamas silence weinstein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[first, lady, melania, trump, visit, infant, o...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>first lady melania trump visit infant opioid t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[breaking, audio, sexual, predator, harvey, we...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>breaking audio sexual predator harvey weinstei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[build, wall]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>build wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[took, hillary, abt, minute, blame, nra, madma...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>took hillary abt minute blame nra madman rampa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[hate, woman, much, president, trump, nominate...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>hate woman much president trump nominated anot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[congrats, kirstjen, nielsen, next, secretary,...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>congrats kirstjen nielsen next secretary homel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[eagle, scout, boy, boy, scout, girl, girl, sc...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>eagle scout boy boy scout girl girl scout chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[eminem, freestyle, prof, celebs, still, learn...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>eminem freestyle prof celebs still learned nob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[agree, boy, scout, allowing, girl, join]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>agree boy scout allowing girl join</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[pres, trump, dropped, nuke, truth, bomb, nfl,...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>pres trump dropped nuke truth bomb nfl suspend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[network, news, become, partisan, distorted, f...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>network news become partisan distorted fake li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[gone, russia, literally, hacked, election, tr...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>gone russia literally hacked election trump ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[clear, say, pok, mon, go, used, mean, evidenc...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>clear say pok mon go used mean evidence pokemo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[thank, elected]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>thank elected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[student, describe, like, conservative, colleg...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>student describe like conservative college cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[eminem, trump, supporter]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>eminem trump supporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[boom, judge, napolitano, call, hillary, bogus...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>boom judge napolitano call hillary bogus claim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[burgess, owen, nfl, protest, one, thing, left...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>burgess owen nfl protest one thing leftist pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33345</th>\n",
       "      <td>[insure, ear]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>insure ear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33346</th>\n",
       "      <td>[look, good]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>look good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33347</th>\n",
       "      <td>[problem]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33348</th>\n",
       "      <td>[haha, na, american]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>haha na american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33349</th>\n",
       "      <td>[make, america, weak, forever]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>make america weak forever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33350</th>\n",
       "      <td>[really, rich]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>really rich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33353</th>\n",
       "      <td>[best]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33356</th>\n",
       "      <td>[man, need, time, history]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>man need time history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33365</th>\n",
       "      <td>[border, unsafe]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>border unsafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33369</th>\n",
       "      <td>[raped, bill, cosby]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>raped bill cosby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33374</th>\n",
       "      <td>[officer, rarely, punished, crime]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>officer rarely punished crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34956</th>\n",
       "      <td>[dog, best, friend]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>dog best friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34988</th>\n",
       "      <td>[patriot, act, actual, year, ago, need]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>patriot act actual year ago need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34995</th>\n",
       "      <td>[cool, body, ice]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>cool body ice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34997</th>\n",
       "      <td>[math]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34998</th>\n",
       "      <td>[like, disco, taste, like, tear]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>like disco taste like tear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34999</th>\n",
       "      <td>[drink, much, goose, turn, goose]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>drink much goose turn goose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35004</th>\n",
       "      <td>[life, similar, bicycle, puncture, fix, cycle]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>life similar bicycle puncture fix cycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35005</th>\n",
       "      <td>[concert, ticket, expensive]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>concert ticket expensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35006</th>\n",
       "      <td>[face, liked, much, better, pretend]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>face liked much better pretend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35007</th>\n",
       "      <td>[beat]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>beat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35012</th>\n",
       "      <td>[im, scared, dark, day, night, little, open, m...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>im scared dark day night little open minded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35015</th>\n",
       "      <td>[strange]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>strange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35016</th>\n",
       "      <td>[husband, nyc, yesterday, never, thought, woul...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>husband nyc yesterday never thought would happen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35018</th>\n",
       "      <td>[relax, also, cannot]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>relax also cannot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35020</th>\n",
       "      <td>[omg, terrorist, android]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>omg terrorist android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35022</th>\n",
       "      <td>[let, teach, hillary, use, email, together, lol]</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>let teach hillary use email together lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36075</th>\n",
       "      <td>[south, florida, mayor, want, talk, climate, c...</td>\n",
       "      <td>LeftTroll</td>\n",
       "      <td>south florida mayor want talk climate change m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36077</th>\n",
       "      <td>[nsa, illegal, spy, program, must, canceled, f...</td>\n",
       "      <td>LeftTroll</td>\n",
       "      <td>nsa illegal spy program must canceled forever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36078</th>\n",
       "      <td>[cutie]</td>\n",
       "      <td>LeftTroll</td>\n",
       "      <td>cutie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>819428 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content account_category  \\\n",
       "0      [sitting, democrat, u, senator, trial, corrupt...       RightTroll   \n",
       "1      [marshawn, lynch, arrives, game, anti, trump, ...       RightTroll   \n",
       "3      [president, trump, dedicates, president, cup, ...       RightTroll   \n",
       "5      [dan, bongino, nobody, troll, liberal, better,...       RightTroll   \n",
       "8      [much, hate, promoting, cnn, article, admittin...       RightTroll   \n",
       "9      [genocide, remark, san, juan, mayor, narrative...       RightTroll   \n",
       "10     [genocide, remark, san, juan, mayor, narrative...       RightTroll   \n",
       "11                                 [apologize, u, lying]       RightTroll   \n",
       "12     [sarah, sander, destroys, nbc, reporter, trump...       RightTroll   \n",
       "13     [hi, remember, said, weinstein, wonderful, hum...       RightTroll   \n",
       "14     [hi, remember, praised, harvey, weinstein, won...       RightTroll   \n",
       "15     [wow, even, cnn, slamming, obamas, silence, we...       RightTroll   \n",
       "16     [first, lady, melania, trump, visit, infant, o...       RightTroll   \n",
       "17     [breaking, audio, sexual, predator, harvey, we...       RightTroll   \n",
       "18                                         [build, wall]       RightTroll   \n",
       "19     [took, hillary, abt, minute, blame, nra, madma...       RightTroll   \n",
       "20     [hate, woman, much, president, trump, nominate...       RightTroll   \n",
       "21     [congrats, kirstjen, nielsen, next, secretary,...       RightTroll   \n",
       "23     [eagle, scout, boy, boy, scout, girl, girl, sc...       RightTroll   \n",
       "24     [eminem, freestyle, prof, celebs, still, learn...       RightTroll   \n",
       "25             [agree, boy, scout, allowing, girl, join]       RightTroll   \n",
       "26     [pres, trump, dropped, nuke, truth, bomb, nfl,...       RightTroll   \n",
       "28     [network, news, become, partisan, distorted, f...       RightTroll   \n",
       "29     [gone, russia, literally, hacked, election, tr...       RightTroll   \n",
       "30     [clear, say, pok, mon, go, used, mean, evidenc...       RightTroll   \n",
       "32                                      [thank, elected]       RightTroll   \n",
       "34     [student, describe, like, conservative, colleg...       RightTroll   \n",
       "35                            [eminem, trump, supporter]       RightTroll   \n",
       "36     [boom, judge, napolitano, call, hillary, bogus...       RightTroll   \n",
       "37     [burgess, owen, nfl, protest, one, thing, left...       RightTroll   \n",
       "...                                                  ...              ...   \n",
       "33345                                      [insure, ear]       RightTroll   \n",
       "33346                                       [look, good]       RightTroll   \n",
       "33347                                          [problem]       RightTroll   \n",
       "33348                               [haha, na, american]       RightTroll   \n",
       "33349                     [make, america, weak, forever]       RightTroll   \n",
       "33350                                     [really, rich]       RightTroll   \n",
       "33353                                             [best]       RightTroll   \n",
       "33356                         [man, need, time, history]       RightTroll   \n",
       "33365                                   [border, unsafe]       RightTroll   \n",
       "33369                               [raped, bill, cosby]       RightTroll   \n",
       "33374                 [officer, rarely, punished, crime]       RightTroll   \n",
       "34956                                [dog, best, friend]       RightTroll   \n",
       "34988            [patriot, act, actual, year, ago, need]       RightTroll   \n",
       "34995                                  [cool, body, ice]       RightTroll   \n",
       "34997                                             [math]       RightTroll   \n",
       "34998                   [like, disco, taste, like, tear]       RightTroll   \n",
       "34999                  [drink, much, goose, turn, goose]       RightTroll   \n",
       "35004     [life, similar, bicycle, puncture, fix, cycle]       RightTroll   \n",
       "35005                       [concert, ticket, expensive]       RightTroll   \n",
       "35006               [face, liked, much, better, pretend]       RightTroll   \n",
       "35007                                             [beat]       RightTroll   \n",
       "35012  [im, scared, dark, day, night, little, open, m...       RightTroll   \n",
       "35015                                          [strange]       RightTroll   \n",
       "35016  [husband, nyc, yesterday, never, thought, woul...       RightTroll   \n",
       "35018                              [relax, also, cannot]       RightTroll   \n",
       "35020                          [omg, terrorist, android]       RightTroll   \n",
       "35022   [let, teach, hillary, use, email, together, lol]       RightTroll   \n",
       "36075  [south, florida, mayor, want, talk, climate, c...        LeftTroll   \n",
       "36077  [nsa, illegal, spy, program, must, canceled, f...        LeftTroll   \n",
       "36078                                            [cutie]        LeftTroll   \n",
       "\n",
       "                                    content_nontokenized  \n",
       "0      sitting democrat u senator trial corruption ba...  \n",
       "1      marshawn lynch arrives game anti trump shirt j...  \n",
       "3      president trump dedicates president cup golf t...  \n",
       "5      dan bongino nobody troll liberal better donald...  \n",
       "8      much hate promoting cnn article admitting ever...  \n",
       "9      genocide remark san juan mayor narrative chang...  \n",
       "10     genocide remark san juan mayor narrative chang...  \n",
       "11                                     apologize u lying  \n",
       "12     sarah sander destroys nbc reporter trump made ...  \n",
       "13     hi remember said weinstein wonderful human goo...  \n",
       "14     hi remember praised harvey weinstein wonderful...  \n",
       "15     wow even cnn slamming obamas silence weinstein...  \n",
       "16     first lady melania trump visit infant opioid t...  \n",
       "17     breaking audio sexual predator harvey weinstei...  \n",
       "18                                            build wall  \n",
       "19     took hillary abt minute blame nra madman rampa...  \n",
       "20     hate woman much president trump nominated anot...  \n",
       "21     congrats kirstjen nielsen next secretary homel...  \n",
       "23     eagle scout boy boy scout girl girl scout chan...  \n",
       "24     eminem freestyle prof celebs still learned nob...  \n",
       "25                    agree boy scout allowing girl join  \n",
       "26     pres trump dropped nuke truth bomb nfl suspend...  \n",
       "28     network news become partisan distorted fake li...  \n",
       "29     gone russia literally hacked election trump ru...  \n",
       "30     clear say pok mon go used mean evidence pokemo...  \n",
       "32                                         thank elected  \n",
       "34     student describe like conservative college cam...  \n",
       "35                                eminem trump supporter  \n",
       "36     boom judge napolitano call hillary bogus claim...  \n",
       "37     burgess owen nfl protest one thing leftist pri...  \n",
       "...                                                  ...  \n",
       "33345                                         insure ear  \n",
       "33346                                          look good  \n",
       "33347                                            problem  \n",
       "33348                                   haha na american  \n",
       "33349                          make america weak forever  \n",
       "33350                                        really rich  \n",
       "33353                                               best  \n",
       "33356                              man need time history  \n",
       "33365                                      border unsafe  \n",
       "33369                                   raped bill cosby  \n",
       "33374                      officer rarely punished crime  \n",
       "34956                                    dog best friend  \n",
       "34988                   patriot act actual year ago need  \n",
       "34995                                      cool body ice  \n",
       "34997                                               math  \n",
       "34998                         like disco taste like tear  \n",
       "34999                        drink much goose turn goose  \n",
       "35004            life similar bicycle puncture fix cycle  \n",
       "35005                           concert ticket expensive  \n",
       "35006                     face liked much better pretend  \n",
       "35007                                               beat  \n",
       "35012        im scared dark day night little open minded  \n",
       "35015                                            strange  \n",
       "35016   husband nyc yesterday never thought would happen  \n",
       "35018                                  relax also cannot  \n",
       "35020                              omg terrorist android  \n",
       "35022           let teach hillary use email together lol  \n",
       "36075  south florida mayor want talk climate change m...  \n",
       "36077      nsa illegal spy program must canceled forever  \n",
       "36078                                              cutie  \n",
       "\n",
       "[819428 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def compute_frequency(df):\n",
    "    vectorizer = CountVectorizer(max_df=0.90,min_df=2, stop_words='english')\n",
    "    word_feature = vectorizer.fit_transform(df['content_nontokenized'])\n",
    "    sum_words = word_feature.sum(axis=0)\n",
    "    words_freq = [(word,sum_words[0,idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x:x[1], reverse=True)\n",
    "    return words_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 1675),\n",
       " ('people', 1342),\n",
       " ('love', 1119),\n",
       " ('make', 1055),\n",
       " ('time', 928),\n",
       " ('day', 857),\n",
       " ('trump', 811),\n",
       " ('want', 756),\n",
       " ('know', 740),\n",
       " ('life', 661),\n",
       " ('think', 659),\n",
       " ('good', 639),\n",
       " ('hate', 617),\n",
       " ('need', 594),\n",
       " ('game', 591),\n",
       " ('really', 557),\n",
       " ('got', 556),\n",
       " ('thing', 556),\n",
       " ('man', 548),\n",
       " ('new', 547),\n",
       " ('say', 546),\n",
       " ('friend', 524),\n",
       " ('let', 489),\n",
       " ('best', 487),\n",
       " ('year', 482),\n",
       " ('look', 466),\n",
       " ('work', 459),\n",
       " ('woman', 445),\n",
       " ('right', 444),\n",
       " ('world', 439),\n",
       " ('president', 422),\n",
       " ('rt', 418),\n",
       " ('girl', 395),\n",
       " ('way', 394),\n",
       " ('better', 392),\n",
       " ('tell', 390),\n",
       " ('white', 385),\n",
       " ('going', 382),\n",
       " ('black', 369),\n",
       " ('stop', 363),\n",
       " ('house', 360),\n",
       " ('twitter', 359),\n",
       " ('come', 351),\n",
       " ('guy', 348),\n",
       " ('eat', 346),\n",
       " ('money', 342),\n",
       " ('dog', 341),\n",
       " ('kid', 338),\n",
       " ('hashtag', 327),\n",
       " ('play', 325),\n",
       " ('oh', 323),\n",
       " ('cat', 322),\n",
       " ('free', 320),\n",
       " ('great', 319),\n",
       " ('men', 305),\n",
       " ('said', 302),\n",
       " ('baby', 302),\n",
       " ('start', 301),\n",
       " ('watch', 296),\n",
       " ('real', 288),\n",
       " ('home', 286),\n",
       " ('tweet', 286),\n",
       " ('big', 283),\n",
       " ('use', 281),\n",
       " ('little', 280),\n",
       " ('bad', 279),\n",
       " ('night', 275),\n",
       " ('mom', 270),\n",
       " ('old', 269),\n",
       " ('actually', 269),\n",
       " ('family', 269),\n",
       " ('shit', 267),\n",
       " ('feel', 264),\n",
       " ('movie', 263),\n",
       " ('food', 259),\n",
       " ('getting', 255),\n",
       " ('god', 254),\n",
       " ('happy', 251),\n",
       " ('phone', 248),\n",
       " ('america', 244),\n",
       " ('hillary', 243),\n",
       " ('gonna', 241),\n",
       " ('long', 240),\n",
       " ('car', 236),\n",
       " ('dead', 236),\n",
       " ('job', 235),\n",
       " ('change', 234),\n",
       " ('child', 233),\n",
       " ('live', 232),\n",
       " ('wait', 229),\n",
       " ('sex', 229),\n",
       " ('word', 227),\n",
       " ('fuck', 227),\n",
       " ('party', 224),\n",
       " ('wife', 223),\n",
       " ('wrong', 222),\n",
       " ('try', 221),\n",
       " ('end', 221),\n",
       " ('sorry', 219),\n",
       " ('obama', 219),\n",
       " ('thought', 218),\n",
       " ('person', 217),\n",
       " ('drink', 211),\n",
       " ('away', 210),\n",
       " ('lot', 210),\n",
       " ('school', 210),\n",
       " ('mean', 208),\n",
       " ('help', 208),\n",
       " ('star', 208),\n",
       " ('hard', 208),\n",
       " ('talk', 207),\n",
       " ('buy', 204),\n",
       " ('dick', 203),\n",
       " ('trying', 202),\n",
       " ('today', 201),\n",
       " ('war', 201),\n",
       " ('problem', 198),\n",
       " ('pizza', 198),\n",
       " ('heart', 197),\n",
       " ('hot', 197),\n",
       " ('run', 193),\n",
       " ('hand', 192),\n",
       " ('leave', 192),\n",
       " ('donald', 191),\n",
       " ('instead', 190),\n",
       " ('christmas', 188),\n",
       " ('american', 187),\n",
       " ('face', 187),\n",
       " ('damn', 187),\n",
       " ('wine', 187),\n",
       " ('boy', 185),\n",
       " ('fun', 185),\n",
       " ('music', 185),\n",
       " ('cause', 184),\n",
       " ('country', 182),\n",
       " ('stupid', 180),\n",
       " ('turn', 180),\n",
       " ('looking', 179),\n",
       " ('lost', 179),\n",
       " ('head', 176),\n",
       " ('wanna', 174),\n",
       " ('sleep', 173),\n",
       " ('win', 172),\n",
       " ('high', 172),\n",
       " ('mother', 172),\n",
       " ('wear', 172),\n",
       " ('dad', 171),\n",
       " ('dream', 171),\n",
       " ('making', 171),\n",
       " ('ask', 168),\n",
       " ('hair', 168),\n",
       " ('watching', 168),\n",
       " ('eating', 168),\n",
       " ('eye', 166),\n",
       " ('started', 164),\n",
       " ('care', 163),\n",
       " ('kill', 163),\n",
       " ('playing', 162),\n",
       " ('sure', 160),\n",
       " ('wall', 159),\n",
       " ('beer', 158),\n",
       " ('book', 158),\n",
       " ('fucking', 158),\n",
       " ('coffee', 158),\n",
       " ('lie', 157),\n",
       " ('morning', 157),\n",
       " ('minute', 157),\n",
       " ('believe', 157),\n",
       " ('hope', 154),\n",
       " ('red', 154),\n",
       " ('tv', 154),\n",
       " ('told', 153),\n",
       " ('join', 152),\n",
       " ('drunk', 152),\n",
       " ('song', 152),\n",
       " ('called', 151),\n",
       " ('vote', 151),\n",
       " ('used', 151),\n",
       " ('remember', 150),\n",
       " ('walk', 150),\n",
       " ('pretty', 148),\n",
       " ('mind', 147),\n",
       " ('fat', 147),\n",
       " ('clinton', 147),\n",
       " ('ball', 147),\n",
       " ('place', 147),\n",
       " ('bed', 147),\n",
       " ('rock', 145),\n",
       " ('harry', 145),\n",
       " ('future', 144),\n",
       " ('body', 143),\n",
       " ('fight', 143),\n",
       " ('read', 143),\n",
       " ('wish', 142),\n",
       " ('story', 142),\n",
       " ('reason', 142),\n",
       " ('pay', 141),\n",
       " ('bos', 139),\n",
       " ('hell', 138),\n",
       " ('street', 138),\n",
       " ('week', 138),\n",
       " ('lol', 138),\n",
       " ('beautiful', 136),\n",
       " ('left', 136),\n",
       " ('hour', 135),\n",
       " ('room', 134),\n",
       " ('stay', 134),\n",
       " ('break', 133),\n",
       " ('parent', 133),\n",
       " ('gay', 133),\n",
       " ('le', 132),\n",
       " ('second', 132),\n",
       " ('brother', 132),\n",
       " ('idea', 131),\n",
       " ('pant', 131),\n",
       " ('joke', 130),\n",
       " ('miss', 130),\n",
       " ('weed', 130),\n",
       " ('office', 130),\n",
       " ('funny', 128),\n",
       " ('state', 126),\n",
       " ('card', 126),\n",
       " ('blue', 126),\n",
       " ('bitch', 126),\n",
       " ('forget', 125),\n",
       " ('running', 125),\n",
       " ('gun', 125),\n",
       " ('king', 124),\n",
       " ('open', 123),\n",
       " ('meet', 122),\n",
       " ('date', 122),\n",
       " ('lose', 121),\n",
       " ('racist', 121),\n",
       " ('power', 120),\n",
       " ('million', 119),\n",
       " ('die', 119),\n",
       " ('yes', 119),\n",
       " ('father', 119),\n",
       " ('death', 118),\n",
       " ('water', 118),\n",
       " ('lady', 118),\n",
       " ('wanted', 117),\n",
       " ('realize', 117),\n",
       " ('fake', 115),\n",
       " ('video', 114),\n",
       " ('drive', 114),\n",
       " ('ex', 114),\n",
       " ('dance', 113),\n",
       " ('box', 113),\n",
       " ('matter', 113),\n",
       " ('nice', 113),\n",
       " ('perfect', 113),\n",
       " ('taking', 112),\n",
       " ('went', 112),\n",
       " ('hurt', 111),\n",
       " ('light', 111),\n",
       " ('green', 110),\n",
       " ('orange', 110),\n",
       " ('law', 109),\n",
       " ('mr', 109),\n",
       " ('picture', 109),\n",
       " ('talking', 109),\n",
       " ('bring', 108),\n",
       " ('trust', 108),\n",
       " ('cool', 108),\n",
       " ('pic', 108),\n",
       " ('walking', 108),\n",
       " ('line', 108),\n",
       " ('hey', 107),\n",
       " ('check', 107),\n",
       " ('shot', 107),\n",
       " ('saying', 107),\n",
       " ('order', 106),\n",
       " ('learn', 105),\n",
       " ('asked', 105),\n",
       " ('politics', 105),\n",
       " ('feeling', 105),\n",
       " ('support', 105),\n",
       " ('human', 104),\n",
       " ('fish', 104),\n",
       " ('pokemon', 104),\n",
       " ('truth', 103),\n",
       " ('news', 103),\n",
       " ('saw', 103),\n",
       " ('cop', 103),\n",
       " ('yeah', 103),\n",
       " ('true', 102),\n",
       " ('probably', 102),\n",
       " ('club', 102),\n",
       " ('summer', 102),\n",
       " ('secret', 102),\n",
       " ('plan', 102),\n",
       " ('stuff', 102),\n",
       " ('single', 102),\n",
       " ('able', 102),\n",
       " ('stand', 101),\n",
       " ('married', 101),\n",
       " ('suck', 101),\n",
       " ('drug', 101),\n",
       " ('jesus', 101),\n",
       " ('took', 100),\n",
       " ('ok', 100),\n",
       " ('son', 100),\n",
       " ('question', 100),\n",
       " ('peace', 99),\n",
       " ('chicken', 99),\n",
       " ('doctor', 99),\n",
       " ('came', 98),\n",
       " ('gone', 98),\n",
       " ('worst', 98),\n",
       " ('drinking', 98),\n",
       " ('im', 98),\n",
       " ('living', 97),\n",
       " ('using', 97),\n",
       " ('point', 97),\n",
       " ('working', 97),\n",
       " ('dinner', 97),\n",
       " ('dude', 96),\n",
       " ('city', 96),\n",
       " ('sound', 96),\n",
       " ('march', 96),\n",
       " ('thinking', 96),\n",
       " ('internet', 96),\n",
       " ('broke', 95),\n",
       " ('coming', 95),\n",
       " ('sister', 95),\n",
       " ('door', 94),\n",
       " ('fact', 94),\n",
       " ('small', 94),\n",
       " ('facebook', 94),\n",
       " ('hear', 94),\n",
       " ('half', 94),\n",
       " ('self', 94),\n",
       " ('adult', 94),\n",
       " ('listen', 93),\n",
       " ('election', 93),\n",
       " ('hit', 93),\n",
       " ('super', 93),\n",
       " ('post', 93),\n",
       " ('smoke', 92),\n",
       " ('maybe', 92),\n",
       " ('understand', 91),\n",
       " ('double', 91),\n",
       " ('finally', 91),\n",
       " ('save', 91),\n",
       " ('monday', 91),\n",
       " ('thanks', 91),\n",
       " ('police', 91),\n",
       " ('decided', 91),\n",
       " ('young', 91),\n",
       " ('smile', 90),\n",
       " ('inside', 90),\n",
       " ('chocolate', 90),\n",
       " ('mad', 90),\n",
       " ('husband', 90),\n",
       " ('account', 90),\n",
       " ('enjoy', 90),\n",
       " ('bar', 89),\n",
       " ('kiss', 89),\n",
       " ('follow', 89),\n",
       " ('photo', 89),\n",
       " ('dirty', 89),\n",
       " ('send', 89),\n",
       " ('tree', 89),\n",
       " ('favorite', 89),\n",
       " ('gift', 88),\n",
       " ('team', 88),\n",
       " ('ice', 88),\n",
       " ('potter', 88),\n",
       " ('anymore', 88),\n",
       " ('seen', 88),\n",
       " ('everybody', 88),\n",
       " ('thank', 88),\n",
       " ('far', 87),\n",
       " ('medium', 87),\n",
       " ('dark', 87),\n",
       " ('bacon', 87),\n",
       " ('animal', 87),\n",
       " ('wearing', 87),\n",
       " ('alcohol', 86),\n",
       " ('weird', 86),\n",
       " ('close', 86),\n",
       " ('waiting', 86),\n",
       " ('clothes', 86),\n",
       " ('month', 86),\n",
       " ('number', 86),\n",
       " ('met', 85),\n",
       " ('throw', 85),\n",
       " ('set', 85),\n",
       " ('moment', 85),\n",
       " ('daughter', 85),\n",
       " ('past', 85),\n",
       " ('friday', 85),\n",
       " ('dress', 85),\n",
       " ('bag', 84),\n",
       " ('rule', 84),\n",
       " ('girlfriend', 84),\n",
       " ('breakfast', 84),\n",
       " ('taco', 84),\n",
       " ('hold', 84),\n",
       " ('public', 83),\n",
       " ('shoe', 83),\n",
       " ('wake', 83),\n",
       " ('heard', 83),\n",
       " ('paper', 83),\n",
       " ('kind', 82),\n",
       " ('gotta', 82),\n",
       " ('short', 82),\n",
       " ('color', 82),\n",
       " ('straight', 82),\n",
       " ('race', 82),\n",
       " ('dont', 81),\n",
       " ('relationship', 81),\n",
       " ('wonder', 81),\n",
       " ('butt', 81),\n",
       " ('earth', 81),\n",
       " ('th', 81),\n",
       " ('cream', 80),\n",
       " ('social', 80),\n",
       " ('deal', 80),\n",
       " ('cut', 79),\n",
       " ('hill', 79),\n",
       " ('special', 79),\n",
       " ('train', 79),\n",
       " ('space', 78),\n",
       " ('isi', 78),\n",
       " ('weekend', 78),\n",
       " ('mouth', 78),\n",
       " ('season', 78),\n",
       " ('killed', 78),\n",
       " ('ready', 78),\n",
       " ('honey', 78),\n",
       " ('sign', 77),\n",
       " ('case', 77),\n",
       " ('smell', 77),\n",
       " ('list', 77),\n",
       " ('sleeping', 77),\n",
       " ('glass', 77),\n",
       " ('outside', 76),\n",
       " ('art', 76),\n",
       " ('fear', 76),\n",
       " ('course', 76),\n",
       " ('late', 76),\n",
       " ('candy', 76),\n",
       " ('fall', 75),\n",
       " ('character', 75),\n",
       " ('sit', 75),\n",
       " ('gold', 75),\n",
       " ('band', 75),\n",
       " ('laugh', 75),\n",
       " ('dollar', 75),\n",
       " ('cuz', 75),\n",
       " ('control', 75),\n",
       " ('toilet', 75),\n",
       " ('machine', 74),\n",
       " ('different', 74),\n",
       " ('class', 74),\n",
       " ('government', 73),\n",
       " ('soul', 73),\n",
       " ('college', 73),\n",
       " ('birthday', 73),\n",
       " ('blood', 73),\n",
       " ('snow', 72),\n",
       " ('spend', 72),\n",
       " ('build', 72),\n",
       " ('ur', 72),\n",
       " ('cake', 72),\n",
       " ('giving', 72),\n",
       " ('share', 72),\n",
       " ('clean', 72),\n",
       " ('john', 72),\n",
       " ('pet', 72),\n",
       " ('born', 72),\n",
       " ('bush', 72),\n",
       " ('hello', 72),\n",
       " ('opinion', 72),\n",
       " ('magic', 71),\n",
       " ('finger', 71),\n",
       " ('sad', 70),\n",
       " ('age', 70),\n",
       " ('history', 70),\n",
       " ('tomorrow', 70),\n",
       " ('bottle', 70),\n",
       " ('bigger', 70),\n",
       " ('soon', 70),\n",
       " ('freedom', 70),\n",
       " ('pick', 70),\n",
       " ('gave', 70),\n",
       " ('sun', 70),\n",
       " ('pain', 69),\n",
       " ('important', 69),\n",
       " ('chance', 69),\n",
       " ('taste', 69),\n",
       " ('political', 69),\n",
       " ('cute', 69),\n",
       " ('wedding', 69),\n",
       " ('return', 69),\n",
       " ('answer', 69),\n",
       " ('business', 69),\n",
       " ('fast', 69),\n",
       " ('act', 69),\n",
       " ('brain', 69),\n",
       " ('cup', 68),\n",
       " ('shop', 68),\n",
       " ('bought', 68),\n",
       " ('roll', 68),\n",
       " ('catch', 68),\n",
       " ('bathroom', 68),\n",
       " ('grow', 68),\n",
       " ('choice', 68),\n",
       " ('store', 68),\n",
       " ('tried', 68),\n",
       " ('netflix', 68),\n",
       " ('selfie', 67),\n",
       " ('duck', 67),\n",
       " ('busy', 67),\n",
       " ('step', 67),\n",
       " ('tear', 67),\n",
       " ('park', 67),\n",
       " ('guess', 67),\n",
       " ('ya', 67),\n",
       " ('sent', 67),\n",
       " ('candidate', 67),\n",
       " ('ride', 67),\n",
       " ('speech', 67),\n",
       " ('evil', 67),\n",
       " ('road', 66),\n",
       " ('sandwich', 66),\n",
       " ('bone', 66),\n",
       " ('beat', 66),\n",
       " ('tonight', 66),\n",
       " ('fine', 65),\n",
       " ('bird', 65),\n",
       " ('button', 65),\n",
       " ('meme', 65),\n",
       " ('reading', 65),\n",
       " ('crazy', 65),\n",
       " ('afraid', 65),\n",
       " ('george', 65),\n",
       " ('beach', 65),\n",
       " ('ticket', 65),\n",
       " ('idiot', 64),\n",
       " ('test', 64),\n",
       " ('later', 64),\n",
       " ('batman', 64),\n",
       " ('angry', 64),\n",
       " ('rich', 64),\n",
       " ('football', 64),\n",
       " ('asshole', 64),\n",
       " ('turned', 64),\n",
       " ('ugly', 64),\n",
       " ('tax', 64),\n",
       " ('forever', 64),\n",
       " ('smoking', 63),\n",
       " ('company', 63),\n",
       " ('pretend', 63),\n",
       " ('strong', 63),\n",
       " ('egg', 63),\n",
       " ('shirt', 63),\n",
       " ('pussy', 63),\n",
       " ('forgot', 62),\n",
       " ('awesome', 62),\n",
       " ('allowed', 62),\n",
       " ('sweet', 62),\n",
       " ('sport', 62),\n",
       " ('yo', 62),\n",
       " ('pray', 62),\n",
       " ('holiday', 62),\n",
       " ('loved', 62),\n",
       " ('write', 62),\n",
       " ('alive', 62),\n",
       " ('cook', 62),\n",
       " ('actor', 62),\n",
       " ('text', 61),\n",
       " ('cold', 61),\n",
       " ('broken', 61),\n",
       " ('sock', 61),\n",
       " ('ppl', 61),\n",
       " ('fart', 61),\n",
       " ('breaking', 61),\n",
       " ('poor', 61),\n",
       " ('died', 61),\n",
       " ('brought', 61),\n",
       " ('meeting', 60),\n",
       " ('mistake', 60),\n",
       " ('porn', 60),\n",
       " ('presidential', 60),\n",
       " ('lord', 60),\n",
       " ('pot', 60),\n",
       " ('wild', 60),\n",
       " ('foot', 60),\n",
       " ('shade', 60),\n",
       " ('realized', 60),\n",
       " ('attention', 60),\n",
       " ('literally', 60),\n",
       " ('tag', 60),\n",
       " ('weight', 60),\n",
       " ('bit', 60),\n",
       " ('begin', 60),\n",
       " ('amazing', 60),\n",
       " ('apparently', 60),\n",
       " ('naked', 60),\n",
       " ('reality', 59),\n",
       " ('selfies', 59),\n",
       " ('queen', 59),\n",
       " ('boyfriend', 59),\n",
       " ('award', 59),\n",
       " ('easy', 59),\n",
       " ('respect', 59),\n",
       " ('finish', 59),\n",
       " ('fan', 58),\n",
       " ('piece', 58),\n",
       " ('politician', 58),\n",
       " ('stick', 58),\n",
       " ('pas', 58),\n",
       " ('okay', 58),\n",
       " ('tired', 58),\n",
       " ('email', 58),\n",
       " ('ago', 58),\n",
       " ('extra', 58),\n",
       " ('asking', 58),\n",
       " ('lying', 58),\n",
       " ('totally', 58),\n",
       " ('caught', 57),\n",
       " ('happen', 57),\n",
       " ('sense', 57),\n",
       " ('safe', 57),\n",
       " ('prince', 57),\n",
       " ('china', 57),\n",
       " ('sunday', 57),\n",
       " ('named', 57),\n",
       " ('wind', 57),\n",
       " ('costume', 57),\n",
       " ('finding', 57),\n",
       " ('jack', 57),\n",
       " ('ring', 57),\n",
       " ('private', 57),\n",
       " ('force', 57),\n",
       " ('dancing', 57),\n",
       " ('apple', 57),\n",
       " ('travel', 57),\n",
       " ('dr', 57),\n",
       " ('somebody', 57),\n",
       " ('happened', 56),\n",
       " ('everyday', 56),\n",
       " ('voice', 56),\n",
       " ('rest', 56),\n",
       " ('count', 56),\n",
       " ('welcome', 56),\n",
       " ('present', 56),\n",
       " ('avoid', 56),\n",
       " ('milk', 56),\n",
       " ('shower', 56),\n",
       " ('condom', 55),\n",
       " ('key', 55),\n",
       " ('non', 55),\n",
       " ('toy', 55),\n",
       " ('killing', 55),\n",
       " ('issue', 55),\n",
       " ('vegan', 55),\n",
       " ('burn', 55),\n",
       " ('bear', 55),\n",
       " ('lunch', 55),\n",
       " ('window', 55),\n",
       " ('driving', 55),\n",
       " ('middle', 55),\n",
       " ('deep', 54),\n",
       " ('hide', 54),\n",
       " ('la', 54),\n",
       " ('cheese', 54),\n",
       " ('penis', 54),\n",
       " ('boo', 54),\n",
       " ('hole', 54),\n",
       " ('size', 54),\n",
       " ('skin', 54),\n",
       " ('telling', 54),\n",
       " ('trending', 53),\n",
       " ('promise', 53),\n",
       " ('captain', 53),\n",
       " ('taken', 53),\n",
       " ('drop', 53),\n",
       " ('tweeting', 53),\n",
       " ('diet', 53),\n",
       " ('unless', 53),\n",
       " ('worth', 53),\n",
       " ('troll', 53),\n",
       " ('knew', 52),\n",
       " ('air', 52),\n",
       " ('west', 52),\n",
       " ('sexy', 52),\n",
       " ('santa', 52),\n",
       " ('view', 52),\n",
       " ('boring', 52),\n",
       " ('pink', 52),\n",
       " ('boob', 52),\n",
       " ('leg', 52),\n",
       " ('bank', 52),\n",
       " ('devil', 51),\n",
       " ('worry', 51),\n",
       " ('saving', 51),\n",
       " ('paid', 51),\n",
       " ('grandma', 51),\n",
       " ('fry', 51),\n",
       " ('sick', 51),\n",
       " ('ate', 51),\n",
       " ('longer', 51),\n",
       " ('series', 51),\n",
       " ('battery', 51),\n",
       " ('trip', 51),\n",
       " ('dumb', 50),\n",
       " ('mary', 50),\n",
       " ('touch', 50),\n",
       " ('personal', 50),\n",
       " ('hungry', 50),\n",
       " ('fly', 50),\n",
       " ('awkward', 50),\n",
       " ('action', 49),\n",
       " ('seriously', 49),\n",
       " ('university', 49),\n",
       " ('liberal', 49),\n",
       " ('bang', 49),\n",
       " ('usa', 49),\n",
       " ('alien', 49),\n",
       " ('canada', 49),\n",
       " ('illegal', 49),\n",
       " ('rain', 49),\n",
       " ('direction', 49),\n",
       " ('nude', 49),\n",
       " ('mexican', 49),\n",
       " ('cover', 49),\n",
       " ('follower', 49),\n",
       " ('bomb', 49),\n",
       " ('sitting', 49),\n",
       " ('refuse', 49),\n",
       " ('ability', 49),\n",
       " ('teeth', 48),\n",
       " ('excuse', 48),\n",
       " ('religion', 48),\n",
       " ('early', 48),\n",
       " ('decision', 48),\n",
       " ('marry', 48),\n",
       " ('handle', 48),\n",
       " ('service', 48),\n",
       " ('letter', 48),\n",
       " ('huge', 48),\n",
       " ('definitely', 48),\n",
       " ('till', 48),\n",
       " ('muslim', 48),\n",
       " ('blow', 48),\n",
       " ('meant', 48),\n",
       " ('keeping', 47),\n",
       " ('worse', 47),\n",
       " ('trouble', 47),\n",
       " ('lead', 47),\n",
       " ('bernie', 47),\n",
       " ('pregnant', 47),\n",
       " ('birth', 47),\n",
       " ('fit', 47),\n",
       " ('daddy', 47),\n",
       " ('burger', 47),\n",
       " ('film', 47),\n",
       " ('google', 47),\n",
       " ('iphone', 47),\n",
       " ('voting', 47),\n",
       " ('meat', 47),\n",
       " ('st', 47),\n",
       " ('jump', 47),\n",
       " ('killer', 47),\n",
       " ('sky', 47),\n",
       " ('lazy', 47),\n",
       " ('happens', 46),\n",
       " ('dating', 46),\n",
       " ('fantastic', 46),\n",
       " ('flag', 46),\n",
       " ('mountain', 46),\n",
       " ('campaign', 46),\n",
       " ('mile', 46),\n",
       " ('land', 46),\n",
       " ('chill', 46),\n",
       " ('ahead', 46),\n",
       " ('brown', 46),\n",
       " ('trash', 46),\n",
       " ('accidentally', 46),\n",
       " ('calling', 46),\n",
       " ('monster', 46),\n",
       " ('watched', 46),\n",
       " ('shopping', 46),\n",
       " ('deserve', 46),\n",
       " ('success', 45),\n",
       " ('terrible', 45),\n",
       " ('education', 45),\n",
       " ('lack', 45),\n",
       " ('suit', 45),\n",
       " ('tea', 45),\n",
       " ('smart', 45),\n",
       " ('legal', 45),\n",
       " ('joe', 45),\n",
       " ('app', 45),\n",
       " ('jail', 45),\n",
       " ('shoot', 45),\n",
       " ('hero', 45),\n",
       " ('marriage', 45),\n",
       " ('plane', 45),\n",
       " ('treat', 45),\n",
       " ('table', 45),\n",
       " ('kick', 45),\n",
       " ('fox', 45),\n",
       " ('national', 45),\n",
       " ('vacation', 45),\n",
       " ('showed', 45),\n",
       " ('singing', 44),\n",
       " ('moon', 44),\n",
       " ('advice', 44),\n",
       " ('exist', 44),\n",
       " ('midnight', 44),\n",
       " ('em', 44),\n",
       " ('choose', 44),\n",
       " ('waste', 44),\n",
       " ('amp', 44),\n",
       " ('prison', 44),\n",
       " ('proud', 44),\n",
       " ('starting', 44),\n",
       " ('mama', 44),\n",
       " ('horse', 44),\n",
       " ('hashtags', 44),\n",
       " ('nap', 44),\n",
       " ('crush', 43),\n",
       " ('speak', 43),\n",
       " ('twice', 43),\n",
       " ('pop', 43),\n",
       " ('absolutely', 43),\n",
       " ('addicted', 43),\n",
       " ('puff', 43),\n",
       " ('wow', 43),\n",
       " ('shut', 43),\n",
       " ('gym', 43),\n",
       " ('nigga', 43),\n",
       " ('beauty', 43),\n",
       " ('stranger', 43),\n",
       " ('forward', 43),\n",
       " ('vodka', 43),\n",
       " ('seeing', 43),\n",
       " ('loving', 43),\n",
       " ('justice', 43),\n",
       " ('falling', 43),\n",
       " ('visit', 43),\n",
       " ('repeat', 43),\n",
       " ('pool', 43),\n",
       " ('type', 42),\n",
       " ('society', 42),\n",
       " ('sell', 42),\n",
       " ('interesting', 42),\n",
       " ('united', 42),\n",
       " ('student', 42),\n",
       " ('planet', 42),\n",
       " ('jean', 42),\n",
       " ('neighbor', 42),\n",
       " ('fail', 42),\n",
       " ('fix', 42),\n",
       " ('throne', 42),\n",
       " ('sexual', 42),\n",
       " ('justin', 42),\n",
       " ('bieber', 42),\n",
       " ('church', 42),\n",
       " ('republican', 42),\n",
       " ('bae', 42),\n",
       " ('kanye', 42),\n",
       " ('message', 42),\n",
       " ('attack', 42),\n",
       " ('trend', 41),\n",
       " ('greatest', 41),\n",
       " ('winter', 41),\n",
       " ('sing', 41),\n",
       " ('health', 41),\n",
       " ('speed', 41),\n",
       " ('lion', 41),\n",
       " ('donut', 41),\n",
       " ('bell', 41),\n",
       " ('outta', 41),\n",
       " ('theory', 41),\n",
       " ('paying', 41),\n",
       " ('random', 41),\n",
       " ('crap', 41),\n",
       " ('needed', 41),\n",
       " ('fridge', 41),\n",
       " ('seven', 40),\n",
       " ('teach', 40),\n",
       " ('heaven', 40),\n",
       " ('simple', 40),\n",
       " ('member', 40),\n",
       " ('cooky', 40),\n",
       " ('wifi', 40),\n",
       " ('terrorist', 40),\n",
       " ('bob', 40),\n",
       " ('leaving', 40),\n",
       " ('useless', 40),\n",
       " ('fired', 40),\n",
       " ('pie', 40),\n",
       " ('popular', 40),\n",
       " ('supposed', 40),\n",
       " ('conversation', 40),\n",
       " ('entire', 40),\n",
       " ('cleaning', 40),\n",
       " ('underwear', 40),\n",
       " ('cooking', 40),\n",
       " ('hang', 40),\n",
       " ('giant', 40),\n",
       " ('pm', 40),\n",
       " ('happiness', 40),\n",
       " ('golden', 40),\n",
       " ('dm', 40),\n",
       " ('stone', 40),\n",
       " ('buying', 40),\n",
       " ('dear', 39),\n",
       " ('common', 39),\n",
       " ('bowl', 39),\n",
       " ('agree', 39),\n",
       " ('computer', 39),\n",
       " ('group', 39),\n",
       " ('cow', 39),\n",
       " ('climate', 39),\n",
       " ('supporter', 39),\n",
       " ('bong', 39),\n",
       " ('knowing', 39),\n",
       " ('kidding', 39),\n",
       " ('murder', 39),\n",
       " ('enemy', 39),\n",
       " ('judge', 39),\n",
       " ('blind', 39),\n",
       " ('superman', 39),\n",
       " ('wonderful', 39),\n",
       " ('town', 39),\n",
       " ('goal', 39),\n",
       " ('lawyer', 39),\n",
       " ('study', 39),\n",
       " ('carry', 39),\n",
       " ('limit', 39),\n",
       " ('awful', 39),\n",
       " ('af', 39),\n",
       " ('piss', 39),\n",
       " ('term', 39),\n",
       " ('strange', 39),\n",
       " ('emmy', 39),\n",
       " ('celebrity', 39),\n",
       " ('butter', 39),\n",
       " ('bride', 39),\n",
       " ('wood', 39),\n",
       " ('grab', 39),\n",
       " ('mon', 39),\n",
       " ('explain', 38),\n",
       " ('racism', 38),\n",
       " ('lawn', 38),\n",
       " ('nut', 38),\n",
       " ('rolling', 38),\n",
       " ('hi', 38),\n",
       " ('york', 38),\n",
       " ('virgin', 38),\n",
       " ('da', 38),\n",
       " ('episode', 38),\n",
       " ('bet', 38),\n",
       " ('fresh', 38),\n",
       " ('sugar', 38),\n",
       " ('online', 38),\n",
       " ('aid', 38),\n",
       " ('empire', 38),\n",
       " ('quit', 38),\n",
       " ('hat', 38),\n",
       " ('blame', 38),\n",
       " ('loser', 38),\n",
       " ('pro', 38),\n",
       " ('charlie', 38),\n",
       " ('steal', 38),\n",
       " ('beard', 38),\n",
       " ('feed', 38),\n",
       " ('leader', 38),\n",
       " ('rid', 37),\n",
       " ('slow', 37),\n",
       " ('wing', 37),\n",
       " ('prefer', 37),\n",
       " ('hug', 37),\n",
       " ('healthy', 37),\n",
       " ('exactly', 37),\n",
       " ('saturday', 37),\n",
       " ('cash', 37),\n",
       " ('style', 37),\n",
       " ('putting', 37),\n",
       " ('moving', 37),\n",
       " ('couch', 37),\n",
       " ('wtf', 37),\n",
       " ('female', 37),\n",
       " ('scared', 37),\n",
       " ('court', 37),\n",
       " ('strike', 37),\n",
       " ('teacher', 37),\n",
       " ('walmart', 37),\n",
       " ('price', 37),\n",
       " ('event', 37),\n",
       " ('arm', 37),\n",
       " ('ran', 37),\n",
       " ('teen', 37),\n",
       " ...]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing frequency count\n",
    "compute_frequency(df_filter[df_filter.account_category ==\"HashtagGamer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "#Wordcloud\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "def create_wordcloud(df):\n",
    "\n",
    "    all_words = ' '.join([text for text in df['content_nontokenized']])\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "#Testing Wordcloud\n",
    "%matplotlib\n",
    "create_wordcloud(df_filter[df_filter.account_category ==\"NewsFeed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sentiment Analysis (Polarity+Subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_filter[\"polarity\"]=\"\"\n",
    "df_filter[\"subjectivity\"]=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def calculate_polarity(row):\n",
    "    analysis = TextBlob(str(row[\"content\"]))\n",
    "    if analysis.sentiment.polarity > 0 :\n",
    "        row[\"polarity\"] = \"positive\"\n",
    "    else:\n",
    "        if analysis.sentiment.polarity == 0:\n",
    "            row[\"polarity\"] = \"neutral\"\n",
    "        else: \n",
    "            row[\"polarity\"] = \"negative\"\n",
    "    return row\n",
    "\n",
    "def calculate_subjectivity(row):\n",
    "    analysis = TextBlob(str(row[\"content\"]))\n",
    "    if analysis.sentiment.subjectivity > 0:\n",
    "        row[\"subjectivity\"] = \"subjective\"\n",
    "    else:\n",
    "        row[\"subjectivity\"] = \"objective\"\n",
    "    return row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#calculating Polarity + Subjectivity\n",
    "df_filter = df_filter.apply(calculate_subjectivity, axis=1)\n",
    "df_filter = df_filter.apply(calculate_polarity, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "#plotting polarity \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt0\n",
    "sentiment_plot = pd.DataFrame()\n",
    "sentiment_plot[\"positive\"] = df_filter[df_filter.polarity == \"positive\"].account_category.value_counts()\n",
    "sentiment_plot[\"neutral\"] = df_filter[df_filter.polarity == \"neutral\"].account_category.value_counts()\n",
    "sentiment_plot[\"negative\"] = df_filter[df_filter.polarity == \"negative\"].account_category.value_counts()\n",
    "sentiment_plot.head()\n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    "# set height of bar\n",
    "bars1 = sentiment_plot[\"positive\"]\n",
    "bars2 = sentiment_plot[\"neutral\"]\n",
    "bars3 = sentiment_plot[\"negative\"]\n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "# Make the plot\n",
    "plt0.bar(r1, bars1, color='springgreen', width=barWidth, edgecolor='white', label='positive')\n",
    "plt0.bar(r2, bars2, color='lightgrey', width=barWidth, edgecolor='white', label='neutral')\n",
    "plt0.bar(r3, bars3, color='tomato', width=barWidth, edgecolor='white', label='negative')\n",
    "# Add xticks on the middle of the group bars\n",
    "plt0.xlabel('group', fontweight='bold')\n",
    "plt0.xticks([r + barWidth for r in range(len(bars1))], sentiment_plot.index.values)\n",
    "# Create legend & Show graphic\n",
    "plt0.legend()\n",
    "plt0.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RightTroll', 'NewsFeed', 'LeftTroll', 'HashtagGamer',\n",
       "       'Fearmonger'], dtype=object)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_plot.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#plotting subjectivity \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt2\n",
    "sentiment_plot = pd.DataFrame()\n",
    "sentiment_plot[\"subjective\"] = df_filter[df_filter.subjectivity == \"subjective\"].account_category.value_counts()\n",
    "sentiment_plot[\"objective\"] = df_filter[df_filter.subjectivity == \"objective\"].account_category.value_counts()\n",
    "sentiment_plot.head()\n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    "# set height of bar\n",
    "bars1 = sentiment_plot[\"subjective\"]\n",
    "bars2 = sentiment_plot[\"objective\"]\n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "# Make the plot\n",
    "plt2.bar(r1, bars1, color='royalblue', width=barWidth, edgecolor='white', label='subjective')\n",
    "plt2.bar(r2, bars2, color='orange', width=barWidth, edgecolor='white', label='objective')\n",
    "# Add xticks on the middle of the group bars\n",
    "plt2.xlabel('group', fontweight='bold')\n",
    "plt2.xticks([r + barWidth for r in range(len(bars1))], sentiment_plot.index.values)\n",
    "# Create legend & Show graphic\n",
    "plt2.legend()\n",
    "plt2.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classification\n",
    "\n",
    "\n",
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x = df_complete.content_nontokenized\n",
    "y = df_complete.account_category\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "#### Linear SMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6662924227816898\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  RightTroll       0.93      0.05      0.10       486\n",
      "  Fearmonger       0.69      0.02      0.03      9696\n",
      "   LeftTroll       0.69      0.29      0.41     34340\n",
      "HashtagGamer       0.66      0.95      0.78     66623\n",
      "    NewsFeed       0.68      0.68      0.68     52741\n",
      "\n",
      "   micro avg       0.67      0.67      0.67    163886\n",
      "   macro avg       0.73      0.40      0.40    163886\n",
      "weighted avg       0.67      0.67      0.62    163886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#sgd_parameters = {\"clf__loss\":[\"hinge\"],\"clf__penalty\":[\"l2\"],\"clf__alpha\":[0.0001,0.001,0.01,0.1,1],\"clf__random_state\":[1],\"clf__max_iter\":[100,500,1000]}\n",
    "\n",
    "sgd = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                ('clf', SGDClassifier(alpha=0.0001, loss=\"hinge\", max_iter=100, penalty=\"l2\", random_state=1))])\n",
    "\n",
    "\n",
    "#grid_sgd = GridSearchCV(sgd,cv = 3, n_jobs = 60, param_grid = sgd_parameters)\n",
    "#grid_sgd.fit(xTrain, yTrain)\n",
    "#y_pred = grid_sgd.predict(xTest)\n",
    "\n",
    "sgd.fit(xTrain,yTrain)\n",
    "y_pred=sgd.predict(xTest)\n",
    "my_tags = df_filter.account_category.unique()\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, yTest))\n",
    "print(classification_report(yTest, y_pred,target_names=my_tags))\n",
    "\n",
    "#print (\"The best parametes are:\\n\",grid_sgd.best_params_,\"The best score is:\\n\",grid_sgd.best_score_)\n",
    "#The best parametes are:\n",
    "# {'clf__alpha': 0.0001, 'clf__loss': 'hinge', 'clf__max_iter': 100, 'clf__penalty': 'l2', 'clf__random_state': 1} The best score is:\n",
    "#accuracy 0.6662924227816898\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7155705795491988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  RightTroll       0.05      0.07      0.06       486\n",
      "  Fearmonger       0.47      0.31      0.38      9696\n",
      "   LeftTroll       0.58      0.58      0.58     34340\n",
      "HashtagGamer       0.82      0.86      0.84     66623\n",
      "    NewsFeed       0.70      0.70      0.70     52741\n",
      "\n",
      "   micro avg       0.72      0.72      0.72    163886\n",
      "   macro avg       0.53      0.50      0.51    163886\n",
      "weighted avg       0.71      0.72      0.71    163886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#nb_params = {\"clf__alpha\":[0.0,0.5,1.0],\"clf__fit_prior\":[0,1]}\n",
    "\n",
    "nb = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "               ('clf', MultinomialNB(alpha=1.0, fit_prior=0))])\n",
    "\n",
    "#grid_nb = GridSearchCV(nb,cv = 3, n_jobs = 60, param_grid = nb_params)\n",
    "#grid_nb.fit(xTrain, yTrain)\n",
    "nb.fit(xTrain,yTrain)\n",
    "y_pred = nb.predict(xTest)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, yTest))\n",
    "print(classification_report(yTest, y_pred,target_names=my_tags))\n",
    "#print (\"The best parametes are:\\n\",grid_nb.best_params_,\"The best score is:\\n\",grid_nb.best_score_)\n",
    "#The best parametes are:\n",
    "# {'clf__alpha': 1.0, 'clf__fit_prior': 0} The best score is:\n",
    "# accuracy 0.7155705795491988\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7351634672882369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  RightTroll       0.75      0.15      0.25       486\n",
      "  Fearmonger       0.53      0.33      0.41      9696\n",
      "   LeftTroll       0.62      0.55      0.59     34340\n",
      "HashtagGamer       0.81      0.90      0.86     66623\n",
      "    NewsFeed       0.71      0.72      0.72     52741\n",
      "\n",
      "   micro avg       0.74      0.74      0.74    163886\n",
      "   macro avg       0.69      0.53      0.56    163886\n",
      "weighted avg       0.72      0.74      0.73    163886\n",
      "\n",
      "The best parametes are:\n",
      " {'logistic__C': 1.5, 'logistic__multi_class': 'multinomial', 'logistic__n_jobs': -1, 'logistic__random_state': 1, 'logistic__solver': 'sag'} The best score is:\n",
      " 0.7310271500529333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#logreg_params = {\"logistic__n_jobs\":[-1],\"logistic__C\":[1.0,1.5,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0],\"logistic__multi_class\":[\"multinomial\"],\"logistic__solver\":[\"newton-cg\",\"sag\",\"saga\",\"lbfgs\"],\"logistic__random_state\":[1]}\n",
    "\n",
    "\n",
    "logreg = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                ('logistic', LogisticRegression(C=1.5, multi_class=\"multinomial\", n_jobs=30, random_state=1, solver=\"sag\"))\n",
    "               ])\n",
    "\n",
    "#grid_logreg = GridSearchCV(logreg,cv = 3, n_jobs = 30, param_grid = logreg_params)\n",
    "#grid_logreg.fit(xTrain, yTrain)\n",
    "logreg.fit(xTrain,yTrain)\n",
    "y_pred = logreg.predict(xTest)\n",
    "print('accuracy %s' % accuracy_score(y_pred, yTest))\n",
    "print(classification_report(yTest, y_pred,target_names=my_tags))\n",
    "#print(\"The best parametes are:\\n\",grid_logreg.best_params_,\"The best score is:\\n\",grid_logreg.best_score_)\n",
    "\n",
    "#The best parametes are:\n",
    "# {'logistic__C': 1.5, 'logistic__multi_class': 'multinomial', 'logistic__n_jobs': -1, 'logistic__random_state': 1, 'logistic__solver': 'sag'} The best score is:\n",
    "# accuracy 0.7351634672882369"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Word2Vec\n",
    "## Creating Neural network corpus on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-cfc401c641c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdownsampling\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#model.init_sims(replace=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_filter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Add sg=1 to get skip-gram model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    784\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"incompatible vector size %d in file %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m                 \u001b[1;31m# TOCONSIDER: maybe mismatched vectors still useful enough to merge (truncating/padding)?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_word_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase_insensitive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy4unknown\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdelete_temporary_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace_word_vectors_with_normalized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \"\"\"Discard parameters that are used in training and score. Use if you're sure you're done training a model.\n\u001b[0m\u001b[0;32m    911\u001b[0m         \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mreplace_word_vectors_with_normalized\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0mvectors\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0monly\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaves\u001b[0m \u001b[0mlots\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;31m!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[0mreport\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'total'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         logger.info(\n\u001b[1;32m--> 553\u001b[1;33m             \u001b[1;34m\"estimated required memory for %i words and %i dimensions: %i bytes\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'total'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m         \"\"\"Build vocabulary from a dictionary of word frequencies.\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[0mBuild\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcontains\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[0mWords\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m \u001b[0municode\u001b[0m \u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m                     \"trim_rule provided, if any, will be ignored.\")\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m     \u001b[1;31m# for backward compatibility (aliases pointing to corresponding variables in trainables, vocabulary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Attribute will be removed in 4.0.0, use self.epochs instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Parameters that effect the quality of the model\n",
    "from gensim.models import Word2Vec\n",
    "num_features = 300# Word vector dimensionality \n",
    "feature_size = 100\n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3\n",
    "#model.init_sims(replace=True)\n",
    "model = Word2Vec(df_filter['content'], min_count=2, size=feature_size) #Add sg=1 to get skip-gram model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('trump')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse size, vocabulary, and individual word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#All words are encoded as vectors with 100 dimensions\n",
    "model.wv['donald']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual and multiple simularity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.similarity(w1='', w2='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "similar_words = {search_term: [item[0] for item in model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['corrupt', 'america', 'bernie', 'tax', 'immigrant', 'elected', 'woman','hillary']}\n",
    "similar_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collapse dimensions and visualize proxemity (similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model to create average word vectors for usage in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# get document level embeddings\n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=df_filter['content'], model=model,\n",
    "                                             num_features=feature_size)\n",
    "w2_df = pd.DataFrame(w2v_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(w2_df, df_filter['account_category'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_naive_dv = KNeighborsClassifier(n_neighbors=3, n_jobs=1, algorithm='brute', metric='cosine' )\n",
    "knn_naive_dv.fit(X_train, y_train)\n",
    "y_pred = knn_naive_dv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating separate Neural network corpuses based on troll category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_filter_rt = df_filter[df_filter.account_category == \"RightTroll\"]\n",
    "df_filter_lt = df_filter[df_filter.account_category == \"LeftTroll\"]\n",
    "df_filter_hg = df_filter[df_filter.account_category == \"HashtagGamer\"]\n",
    "df_filter_fm = df_filter[df_filter.account_category == \"Fearmonger\"]\n",
    "\n",
    "model_rt = model(df_filter_rt['content'] , min_count=2)\n",
    "model_lt = model(df_filter_lt['content'] , min_count=2)\n",
    "model_hg = model(df_filter_hg['content'] , min_count=2)\n",
    "model_fm = model(df_filter_fm['content'] , min_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Tweets based on category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cat_count = df_filter['account_category'].value_counts()\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.bar(cat_count.index, cat_count.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See most tweets per category and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_filter['author'].groupby(df_filter['account_category']).value_counts().nlargest(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_filter['account_category'].groupby(df_filter['author']).value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "819428"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter.polarity.value_counts().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 819428 entries, 0 to 36078\n",
      "Data columns (total 3 columns):\n",
      "content                 819428 non-null object\n",
      "account_category        819428 non-null object\n",
      "content_nontokenized    819428 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 25.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_filter.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
